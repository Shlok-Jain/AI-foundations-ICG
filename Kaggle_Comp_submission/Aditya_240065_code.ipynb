{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d1aHs2Wfom-d"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZnu5rhUo7yw"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./train.csv')\n",
    "x_train = np.array(df.iloc[:,1:]) # array, each element being an array of 784 elements, need to be rearranged in a 28*28 array for the image\n",
    "y_train = np.array(df.iloc[:,0]) # labels of the pictures\n",
    "print(x_train.shape)\n",
    "num_samples = x_train.shape[0]\n",
    "X_train = x_train.reshape(num_samples,28,28)\n",
    "# Normalising the dataset\n",
    "X_train = X_train / 255\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "afIphYiQNnRt"
   },
   "outputs": [],
   "source": [
    "print(np.unique(y_train))\n",
    "#Checking the number of unique labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2afWii7CV1QB"
   },
   "outputs": [],
   "source": [
    "# Creating Validation data and training data\n",
    "X_train = X_train.reshape(-1,28,28,1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "# using this because we want the validation data to be randomly taken from the dataset, we don't want to have any kind of relation in the data, knowingly or unknowingly\n",
    "inp_train, inp_validation, labels_train, labels_validation = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42  # 20% validation set\n",
    ")\n",
    "print(inp_train.shape, inp_validation.shape,labels_train.shape, labels_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "10-1vf6vGgb8"
   },
   "outputs": [],
   "source": [
    "# Defining the architecture of the model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def build_model(input_shape=(28, 28, 1), num_classes=25):  # num_classes give the total number of outputs\n",
    "    model = Sequential()\n",
    "\n",
    "    # Convolutional Block 1\n",
    "    model.add(Conv2D(16, (3, 3), activation='relu', input_shape=input_shape, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "\n",
    "    # Convolutional Block 2\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "    # Fully Connected Layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Example: Build and summarize the model\n",
    "model = build_model()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d9yMakEx-PQ1"
   },
   "outputs": [],
   "source": [
    "#Compiling the model using adam\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',  # Use sparse if labels are integers, categorical if one-hot encoded\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T4X1ycElALIl"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor validation loss\n",
    "    patience=1,          # Stop after 1 epochs without improvement\n",
    "    restore_best_weights=True  # Restore the best weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9I5dFmZqJiBa"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    inp_train, labels_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=(inp_validation,labels_validation),\n",
    "    callbacks = [early_stopping]\n",
    ")\n",
    "\n",
    "def plot_training_history(history):\n",
    "    # Extract values from the history object\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    # Create subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Plot accuracy\n",
    "    ax1.plot(acc, label='Training Accuracy')\n",
    "    ax1.plot(val_acc, label='Validation Accuracy')\n",
    "    ax1.set_title('Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "\n",
    "    # Plot loss\n",
    "    ax2.plot(loss, label='Training Loss')\n",
    "    ax2.plot(val_loss, label='Validation Loss')\n",
    "    ax2.set_title('Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to plot the training history\n",
    "plot_training_history(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NfYcap_XiC-h"
   },
   "outputs": [],
   "source": [
    "#Now preparing the testing data\n",
    "df2 = pd.read_csv('./test.csv')\n",
    "\n",
    "x_test = np.array(df2.iloc[:,1:]) # array, each element being an array of 784 elements, need to be rearranged in a 28*28 array for the image\n",
    "y_test = np.array(df2.iloc[:,0]) # id of the pictures\n",
    "\n",
    "num_samples = x_test.shape[0]\n",
    "X_test = x_test.reshape(num_samples,28,28)\n",
    "\n",
    "# Normalising the dataset\n",
    "X_test = X_test / 255\n",
    "X_test= X_test.reshape(-1,28,28,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6dDElC_tkx6J"
   },
   "outputs": [],
   "source": [
    "# Making the predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Get the class with the highest probability for each image\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-B4fMw6ElV45"
   },
   "outputs": [],
   "source": [
    "#Writing the got predictions in a csv file\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with image indices and their predicted classes\n",
    "prediction_df = pd.DataFrame({\n",
    "    \"id\": range(len(predicted_classes)),\n",
    "    \"Predicted Class\": predicted_classes\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "prediction_df.to_csv(\"this_is_new_prediction3.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
